using System;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

using Microsoft.Content.Recommendations.Common;
using Microsoft.Content.Recommendations.LinearAlgebra;
using Newtonsoft.Json;
using Newtonsoft.Json.Linq;
    
namespace Microsoft.Content.TopicExtraction
{
    using System.Reflection;

    using Microsoft.Content.Recommendations;

    public enum MetricType {Coherence, Specificity, Distinctiveness};
    
    public class LDAMetrics
    {
        private const int DataFileVersion = 4;

        private const double DefaultTCEpsilon = 0.000000000001;

        private const string BinFileExt = ".bin";

        private const string WordDocsListMapFileName = "WordDocsListMap.bin";

        private const string TopicsInfoFileName = "TopicsInfo.txt";

        /// <summary>
        /// This is used to decide if a topic (in the sorted list) is bad or not 
        /// by comparing its aggregated allocation with the allocation of the last topic in the list.
        /// </summary>
        private const double MaxDiffOfAllocationBetweenBadTopics = 0.5;

        /// <summary>
        /// Lines of header in topic words allocation file
        /// </summary>
        private const int TopicWordsAllocationHeaderLines = 11;

        /// <summary>
        /// # of columns in document topic allocation file (generated by DvGen)
        /// This constant has dependency on method GenerateLdaDocumentTopicVectors().
        /// </summary>
        private const int NumOfColumnsForDvGenOutput = 4;

        /// <summary>
        /// Corpus vocabularies for look up
        /// </summary>
        private CorpusVocabulary corpusVocabulary;

        /// <summary>
        /// Weight of vocabularies for each topics
        /// Note: don't change the structure
        /// </summary>
        private float[][] topicWords;

        /// <summary>
        /// Documents list for each vocabularies
        /// </summary>
        private int[][] wordDocsListMap;

        /// <summary>
        /// TF for each vocabularies
        /// </summary>
        private int[] wordTF;

        /// <summary>
        /// Global term frequency
        /// </summary>
        private int totalTF;

        /// <summary>
        /// Topics info
        /// </summary>
        private TopicInfo[] topicsInfo;

        private double[,] topicMetrics;

        private double avgTopicCoherence;

        private double avgGoodTopicCoherence;

        private double avgTopicSpecificity;

        private double avgGoodTopicSpecificity;

        private double avgTopicDistinctiveness;

        private double avgGoodTopicDistinctiveness;

        public LDAMetrics()
        {
        }

        public LDAMetrics(int numWords)
        {
            this.NumWords = numWords;
        }

        /// <summary>
        /// Get model topics count
        /// </summary>
        public int NumTopics { get; private set; }

        /// <summary>
        /// Get model vocabularies count
        /// </summary>
        public int NumWords { get; private set; }

        private string documentTopicAllocationsFile;

        private string wordTopicAllocationsFileName;
            
        private int numDocs;
        
        private int numPasses;

        /// <summary>
        /// Initialize module and generate data files if not exist
        /// </summary>
        /// <param name="modelConfig">model config file</param>
        /// <returns>true if successful</returns>
        public bool Initialize(LDAConfig modelConfig)
        {
            // initialize vocabularies
            StatusMessage.Write("Loading corpus vocabulary file...");

            this.corpusVocabulary = CorpusVocabulary.Load(modelConfig.CorpusVocabulary);
            if (this.corpusVocabulary == null)
            {
                return false;
            }

            this.NumTopics = modelConfig.LDAParameters.NumTopics;
            this.NumWords = modelConfig.ModelStatistics.VocabularySize;
            this.documentTopicAllocationsFile = modelConfig.DocumentTopicAllocations;
            this.numDocs = modelConfig.ModelStatistics.DocumentCount;
            this.numPasses = modelConfig.LDAParameters.Passes;

            // Initialize array of topic metrics:  Coherence, Specificity, Distinctiveness
            this.topicMetrics = new double[this.NumTopics, sizeof(MetricType)];
            
            // sanity check
            if (this.NumWords != this.corpusVocabulary.Count())
            {
                StatusMessage.Write("Number of vocabularies mismatch. Check your parameters.");
                return false;
            }
            
            // initialize topic words allocation
            this.wordTopicAllocationsFileName = modelConfig.WordTopicAllocations;
            string topicWordsAllocBinFile = wordTopicAllocationsFileName + BinFileExt;
            
            StatusMessage.Write("Loading topic words allocation bin file...");
            if (!this.LoadTopicWords(topicWordsAllocBinFile))
            {
                StatusMessage.Write("Generating topic words allocation bin file...");
                if (!this.BuildTopicWordsAllocation(this.wordTopicAllocationsFileName, topicWordsAllocBinFile))
            {
                return false;
            }
            }

            // initialize words documents list map
            string wordDocsListMapFile = Path.GetDirectoryName(modelConfig.FeaturizedDocuments) + @"\" + WordDocsListMapFileName;
            
            StatusMessage.Write("Loading word documents list bin file...");
            if (!this.LoadWordDocsListMap(wordDocsListMapFile))
            {
                StatusMessage.Write("Generating word documents list bin file...");
                if (!this.BuildWordDocsListMap(modelConfig.FeaturizedDocuments, wordDocsListMapFile))
                {
                    return false;
                }
            }

            // init topics info
            var topicsInfoFile = Path.GetDirectoryName(modelConfig.DocumentTopicAllocations) + @"\" + TopicsInfoFileName;
            if (!this.LoadTopicsInfo(topicsInfoFile))
            {
                if (!this.BuildTopicsInfo(topicsInfoFile))
            {
                return false;
            }
            }

            return true;
        }

        /// <summary>
        /// Get top words for a topic
        /// </summary>
        /// <param name="topicId">topic id</param>
        /// <param name="topWordsCount">top N words to get</param>
        /// <returns>list of top words for a topic</returns>
        public List<int> GetTopWords(int topicId, int topWordsCount)
        {
            if (topicId >= this.topicWords.Length)
            {
                return null;
            }

            return Enumerable.Range(0, this.NumWords)
                .OrderByDescending(x => this.topicWords[topicId][x])
                .Take(topWordsCount)
                .ToList();
        }

        /// <summary>
        /// Calculate the average topic coherence for top N words
        /// </summary>;
        /// <param name="topWordsCount">top N words</param>
        /// <param name="epsilon">smoothing factor, -1 to use default value</param>
        /// <returns>average TC</returns>
        public double ComputeAvgTopicsCoherence(int topWordsCount, double epsilon)
        {
            this.avgTopicCoherence = 0;
            this.avgGoodTopicCoherence = 0;

            this.ComputeTopicCoherence(topWordsCount, (epsilon < 0) ? DefaultTCEpsilon : epsilon);

            for (int i = 0; i < this.NumTopics; i++)
            {
                this.avgTopicCoherence += this.topicMetrics[i, (int)MetricType.Coherence];

                if (!this.topicsInfo[i].IsBadTopic)
            {
                    this.avgGoodTopicCoherence += this.topicMetrics[i, (int)MetricType.Coherence];
                }
            }

            this.avgTopicCoherence /= this.NumTopics;
            this.avgGoodTopicCoherence /= this.topicsInfo.Count(x => !x.IsBadTopic);

            return this.avgTopicCoherence;
        }

        /// <summary>
        /// Calculate topic specificity for each topic and return the average TS
        /// TS for bad topic is treated as 0
        /// </summary>
        /// <returns>average TS</returns>
        public double ComputeAvgTopicSpecificity()
        {
            var sum = this.topicsInfo
                .Select((t, index) => (t.IsBadTopic || t.ProminentFrequency == 0) ? 0 : this.topicMetrics[index, (int)MetricType.Specificity] = t.ProminentAllocation / t.ProminentFrequency / t.ProminentFrequency)
                .Sum();

            this.avgTopicSpecificity = sum / this.NumTopics;
            this.avgGoodTopicSpecificity = sum / this.topicsInfo.Count(x => !x.IsBadTopic && x.ProminentFrequency > 0);

            return this.avgTopicSpecificity;
        }

        /// <summary>
        /// Calculate topic distinctiveness for each topic and return the average TD
        /// TD for bad topic is treated as 0
        /// </summary>
        /// <returns>average TD</returns>
        public double ComputeAvgTopicDistinctiveness()
        {
            var globalDist = this.wordTF.Select(x => (float)x / (float)this.totalTF).ToArray();

            DenseVector globalDistVector = new DenseVector(globalDist);

            for (int i = 0; i < this.NumTopics; i++)
            {
                if (this.topicsInfo[i].IsBadTopic)
                {
                    continue;
                }

                var topicDist = this.topicWords[i];

                DenseVector topicDistVector = new DenseVector(topicDist);

                var td = 1 - VectorBase.CosineSimilarity(topicDistVector, globalDistVector);

                this.topicMetrics[i, (int)MetricType.Distinctiveness] = td;
                this.avgTopicDistinctiveness += td;
            }

            this.avgGoodTopicDistinctiveness = this.avgTopicDistinctiveness / this.topicsInfo.Count(x => !x.IsBadTopic);
            this.avgTopicDistinctiveness = this.avgTopicDistinctiveness / this.NumTopics;

            return this.avgTopicDistinctiveness;
        }

        /// <summary>
        /// Calculate topic coherence for a specific topic or all topics
        /// </summary>
        /// <param name="topicId">topic to calculate or -1 for all topics</param>
        /// <param name="topWordsCount">take top N words</param>
        /// <param name="epsilon">smoothing factor</param>
        /// <returns></returns>
        private void ComputeTopicCoherence(int topWordsCount, double epsilon = DefaultTCEpsilon)
        {
            int numTopics = this.NumTopics;

            List<Tuple<int, int>>[] topicsWordPairs = new List<Tuple<int, int>>[numTopics];

            for (int topic = 0; topic < numTopics; topic++)
            {
                // get top words for this topic
                var wordIds = this.GetTopWords(topic, topWordsCount);

                // word-pair generation
                List<Tuple<int, int>> wordPairs = new List<Tuple<int, int>>();

                for (int i = 0; i < topWordsCount; i++)
                {
                    for (int j = i + 1; j < topWordsCount; j++)
                    {
                        wordPairs.Add(new Tuple<int, int>(wordIds[i], wordIds[j]));
                    }
                }

                topicsWordPairs[topic] = wordPairs;

                System.Console.Write(".");
            }

            System.Console.WriteLine();

            // compress word pairs and create record for storing frequency
            Dictionary<int, List<WordDocFrequency>> coWordsDFSet = new Dictionary<int, List<WordDocFrequency>>();

            var allWordPairs = topicsWordPairs.SelectMany(x => x);
            foreach (var wordPair in allWordPairs)
            {
                if (coWordsDFSet.ContainsKey(wordPair.Item1))
                {
                    if (!coWordsDFSet[wordPair.Item1].Any(x => x.WordId == wordPair.Item2))
                    {
                        coWordsDFSet[wordPair.Item1].Add(new WordDocFrequency(wordPair.Item2, 0));
                    }
                }
                else
                {
                    coWordsDFSet.Add(wordPair.Item1, new List<WordDocFrequency>() { new WordDocFrequency(wordPair.Item2, 0) });
                }
            }

            System.Console.WriteLine("Total word pairs {0} compressed word pairs {1}", topicsWordPairs[0].Count * numTopics, coWordsDFSet.Sum(x => x.Value.Count));

            // get frequency
            this.GetWordPairsDocumentFrequency(coWordsDFSet);

            // compute topic coherence
            for (int topic = 0; topic < numTopics; topic++)
            {
                var wordPairs = topicsWordPairs[topic];

                for (int pairIdx = 0; pairIdx < wordPairs.Count; pairIdx++)
                {
                    var wordPair = wordPairs[pairIdx];
                    int currentWordDF = this.corpusVocabulary.DocumentFrequency(wordPair.Item1);
                    var coWordDF = coWordsDFSet[wordPair.Item1].FirstOrDefault(x => x.WordId == wordPair.Item2);

                    if (coWordDF != null)
                    {
                        this.topicMetrics[topic, 0] = this.topicMetrics[topic, 0] + Math.Log((coWordDF.Frequency + epsilon) / currentWordDF);
                    }
                }
            }
        }
        
        /// <summary>
        /// Get document frequency for the given compressed word pairs
        /// </summary>
        /// <param name="coWordsDFSet">word pairs map</param>
        private void GetWordPairsDocumentFrequency(Dictionary<int, List<WordDocFrequency>> coWordsDFSet)
        {
            foreach (var key in coWordsDFSet.Keys)
            {
                var keyDocsList = this.wordDocsListMap[key];

                var coWords = coWordsDFSet[key];
                for (int wordIdx = 0; wordIdx < coWords.Count; wordIdx++)
                {
                    var targetDocsList = this.wordDocsListMap[coWords[wordIdx].WordId];
                    coWords[wordIdx].Frequency = this.GetMatchedCount(keyDocsList, targetDocsList);
                }
            }
        }

        /// <summary>
        /// Get matched count count given two ordered list
        /// </summary>
        /// <param name="keyDocsList">ordered list of doc id</param>
        /// <param name="targetDocsList">ordered list of doc id</param>
        /// <returns>matched doc id count</returns>
        private int GetMatchedCount(int[] keyDocsList, int[] targetDocsList)
        {
            int count = 0;
            int i = 0;
            int j = 0;

            while (i < keyDocsList.Length && j < targetDocsList.Length)
            {
                if (keyDocsList[i] == targetDocsList[j])
                {
                    count++;
                    i++;
                    j++;
                }
                else if (targetDocsList[j] < keyDocsList[i])
                {
                    j++;
                }
                else
                {
                    i++;
                }
            }

            return count;
        }

        /// <summary>
        /// Process documents topic allocations file to gather:
        ///     * allocations per topic
        ///     * most prominent allocations per topic
        ///     * number of most prominent documents per topic
        ///     * top 10 documents for each topic
        ///     * identified bad topic
        /// </summary>
        /// <param name="topicsInfoFile">file to store topics info</param>
        /// <returns>true if succeeded</returns>
        private bool BuildTopicsInfo(string topicsInfoFile)
        {
            var docIndex = 0;
            StatusMessage.Write("Aggregating topic document allocations...");
            var writer = StatusMessage.StatusWriter(() => string.Format("Processed {0:D0} documents.", docIndex));
            StatusMessage.Add(writer);

            this.topicsInfo = new TopicInfo[this.NumTopics].Init();

            // Differentiate if the document-topic allocations file was generated by training OR DvGen (and then copy).
            // the one generated by training has the number of columns equal to NumOfTopics, whereas the one generated by DvGen has fixed number of columns = 4.
            string sourceOfDocumentTopicAllocationsFile;            

            var lines =
                File.ReadLines(this.documentTopicAllocationsFile);

            if (lines.Count() < this.numDocs)
            {
                throw new Exception("The number of lines in document-topic allocations file is less than the number of documents!");
            }

            string[] firstLine = lines.First().Split(' ');
            float number;
            if (firstLine.Length == NumOfColumnsForDvGenOutput &&
                !Single.TryParse(firstLine[1], out number) &&
                !Single.TryParse(firstLine[2], out number))
            {
                sourceOfDocumentTopicAllocationsFile = "DvGen";
            }
            else if (firstLine.Length >= NumTopics &&
                     Single.TryParse(firstLine[0], out number) &&
                     Single.TryParse(firstLine[1], out number))            
            {
                sourceOfDocumentTopicAllocationsFile = "Training";
                lines = lines.Skip(this.numDocs * (this.numPasses - 1));                
            }
            else
            {
                throw new Exception("Document-topic allocations file is not in right format!");
            }

            foreach (var line in lines)
            {
                // check if line is empty (this happens when a document does not contain any word in the corpus vocabulary).
                if (string.IsNullOrWhiteSpace(line))
                {
                    docIndex++;
                    continue;
                }

                IEnumerable<float> parts;
                if (sourceOfDocumentTopicAllocationsFile == "Training")
                {
                    parts = line.Split(' ')
                    .Take(this.NumTopics)
                    .Select(Single.Parse);                
                }
                else
                {
                    var denseVector = LoadDocumentVector(line);
                    parts = denseVector.ToArray();                
                }

                var sum = parts.Sum();

                var allocations = parts.Select(v => v / sum).ToList();
                double max = -1;
                List<int> maxTopics = new List<int>();

                for (var topic = 0; topic < this.NumTopics; topic++)
                {
                    var alloc = allocations[topic];

                    this.topicsInfo[topic].AggregatedAllocation += alloc;

                    if (alloc >= max)
                    {
                        if (alloc > max)
                        {
                            max = alloc;
                            maxTopics.Clear();
                        }

                        maxTopics.Add(topic);
                    }
                }

                foreach (var maxTopic in maxTopics)
                {
                    this.topicsInfo[maxTopic].ProminentAllocation += max;
                    this.topicsInfo[maxTopic].ProminentFrequency++;
                    this.topicsInfo[maxTopic].TopProminentDocuments.Add(new Tuple<int, double>(docIndex, max));
                }

                docIndex++;
            }

            foreach (var topicInfo in this.topicsInfo)
            {
                topicInfo.TopProminentDocuments = topicInfo.TopProminentDocuments.OrderByDescending(x => x.Item2).Take(20).ToList();
            }

            var sortedTopics = Enumerable.Range(0, this.NumTopics)
                .OrderByDescending(topic => this.topicsInfo[topic].AggregatedAllocation).ToArray();

            int badTopicCount = 0;
            var lastAllocation = this.topicsInfo[sortedTopics.Last()].AggregatedAllocation;

            for (int i = sortedTopics.Length - 1; i >= 0; i--)
            {
                if (Math.Abs(this.topicsInfo[sortedTopics[i]].AggregatedAllocation - lastAllocation) > MaxDiffOfAllocationBetweenBadTopics)
                {
                    break;
                }

                this.topicsInfo[sortedTopics[i]].IsBadTopic = true;
                badTopicCount++;
            }

            // assume no bad topic
            if (badTopicCount == 1)
            {
                this.topicsInfo[sortedTopics.Last()].IsBadTopic = false;
            }

            using (var sw = new StreamWriter(topicsInfoFile))
            {
                sw.WriteLine(JsonConvert.SerializeObject(this.topicsInfo, Formatting.Indented));
            }

            StatusMessage.Remove(writer);

            return true;
        }

        /// <summary>
        /// Load the specified topics info file
        /// </summary>
        /// <param name="topicsInfoFile">topics info file created by BuildTopicsInfo</param>
        /// <returns>true if loaded</returns>
        private bool LoadTopicsInfo(string topicsInfoFile)
        {
            if (!File.Exists(topicsInfoFile))
            {
                return false;
            }

            try
            {
                this.topicsInfo = JsonConvert.DeserializeObject<TopicInfo[]>(File.ReadAllText(topicsInfoFile));
            }
            catch (Exception e)
            {
                StatusMessage.Write(e.Message);
                return false;
            }

            return true;
        }

        /// <summary>
        /// Generate list of documents id for corpus vocabularies and save to a binary file
        /// 
        /// Binary Word Documents List Format
        ///     INT32 - version
        ///     INT32 - number of vocabularies
        ///     INT32 - documents count for word id 0
        ///     INT32 - TF for word id 0
        ///     INT32[documents count] - document ids for word id 0
        ///     INT32 - documents count for word id 1
        ///     INT32 - TF for word id 1
        ///     INT32[documents count] - document ids for word id 1
        ///     ...
        ///     INT32 - documents count for word id N
        ///     INT32 - TF for word id N
        ///     INT32[documents count] - document ids for word id N
        /// </summary>
        /// <param name="docVocabularyFile">document vocabulary file</param>
        /// <param name="wordDocsListMapFile">binary file to save the mapping</param>
        /// 
        private bool BuildWordDocsListMap(string featurizedDocFile, string wordDocsListMapFile)
        {
            if (!File.Exists(featurizedDocFile))
            {
                StatusMessage.Write(string.Format("File {0} not found", featurizedDocFile));
                return false;
            }

            this.wordTF = new int[this.NumWords];
            this.totalTF = 0;

            var tmpWordDocsListMap = new List<int>[this.NumWords];

            try
            {
                using (var sr = new StreamReader(featurizedDocFile))
                {
                    int currentDoc = 0;

                    while (!sr.EndOfStream)
                    {
                        var docwords = sr.ReadLine();
                        if (string.IsNullOrEmpty(docwords))
                        {
                            break;
                        }

                        var words = docwords.Split(' ').Skip(1);
                        foreach (var word in words)
                        {
                            int sepIdx = word.IndexOf(':');

                            var wordId = Convert.ToInt32(word.Substring(0, sepIdx));
                            var wordFreq = Convert.ToInt32(word.Substring(sepIdx + 1));

                            if (tmpWordDocsListMap[wordId] != null)
                            {
                                tmpWordDocsListMap[wordId].Add(currentDoc);
                            }
                            else
                            {
                                tmpWordDocsListMap[wordId] = new List<int>() { currentDoc };
                            }

                            this.wordTF[wordId] += wordFreq;
                            this.totalTF += wordFreq;
                        }

                        currentDoc++;
                    }
                }

                using (var binWriter = new BinaryWriter(File.Create(wordDocsListMapFile)))
                {
                    binWriter.Write(DataFileVersion);

                    binWriter.Write(tmpWordDocsListMap.Length);

                    int wordId = 0;

                    foreach (var wordDocs in tmpWordDocsListMap)
                    {
                        binWriter.Write(wordDocs.Count);
                        binWriter.Write(this.wordTF[wordId]);

                        foreach (var docId in wordDocs)
                        {
                            binWriter.Write(docId);
                        }

                        wordId++;
                    }
                }

                this.wordDocsListMap = new int[this.NumWords][];
                for (int i = 0; i < tmpWordDocsListMap.Length; i++)
                {
                    this.wordDocsListMap[i] = tmpWordDocsListMap[i].ToArray();
            }
            }
            catch (Exception e)
            {
                System.Console.WriteLine("Error processing document vocabulary file: {0}", e.Message);
                return false;
            }

            return true;
        }

        /// <summary>
        /// Overload previous method BuildWordDocsListMap(string, string)
        /// </summary>
        /// <param name="featurizedDocFile">the path of featurized documents (for a particular <min,max>)</param>
        /// <returns>true if success, false otherwise</returns>
        public bool BuildWordDocsListMapIfNotExists(string featurizedDocFile)
        {
            string wordDocsListMapFile = Path.GetDirectoryName(featurizedDocFile) + @"\" + WordDocsListMapFileName;
            if (File.Exists(wordDocsListMapFile) && FileManager.GetFileLength(wordDocsListMapFile) > 0L)
            {
                StatusMessage.Write("Skipping, word documents list bin file already exists.");
                return true;
            }
            return BuildWordDocsListMap(featurizedDocFile, wordDocsListMapFile);
        }

        /// <summary>
        /// Load documents id list for corpus vocabularies
        /// </summary>
        /// <param name="wordDocsListMapFile">binary file for the mapping</param>
        /// <returns>true if succeeded</returns>
        private bool LoadWordDocsListMap(string wordDocsListMapFile)
        {
            if (!File.Exists(wordDocsListMapFile))
            {
                return false;
            }

            try
            {
                using (var binReader = new BinaryReader(File.OpenRead(wordDocsListMapFile)))
                {
                    int version = binReader.ReadInt32();
                    if (version != DataFileVersion)
                    {
                        return false;
                    }

                    int wordsCount = binReader.ReadInt32();

                    this.totalTF = 0;
                    this.wordTF = new int[wordsCount];
                    this.wordDocsListMap = new int[wordsCount][];

                    for (int i = 0; i < wordsCount; i++)
                    {
                        var docsCount = binReader.ReadInt32();
                        int[] docs = new int[docsCount];

                        var tf = binReader.ReadInt32();
                        this.wordTF[i] = tf;
                        this.totalTF += tf;

                        for (int j = 0; j < docsCount; j++)
                        {
                            docs[j] = binReader.ReadInt32();
                        }

                        this.wordDocsListMap[i] = docs;
                    }
                }
            }
            catch (Exception e)
            {
                StatusMessage.Write("Error reading words documents list file: " + e.Message);
                return false;
            }

            return true;
        }

        /// <summary>
        /// Generate binary topic words allocation file
        /// 
        /// Binary Topic Words Allocation Format
        ///     INT32 - version
        ///     INT32 - number of topics
        ///     INT32 - number of vocabularies
        ///     double[number of vocabularies] - weight of vocabularies for topic 0
        ///     double[number of vocabularies] - weight of vocabularies for topic 1
        ///     ...
        ///     double[number of vocabularies] - weight of vocabularies for topic N
        /// </summary>
        /// <param name="topicWordsAllocFile">text topic words allocation file</param>
        /// <param name="topicWordsAllocBinFile">binary topic words allocation file</param>
        private bool BuildTopicWordsAllocation(string topicWordsAllocFile, string topicWordsAllocBinFile)
        {
            if (!File.Exists(topicWordsAllocFile))
            {
                StatusMessage.Write(string.Format("File {0} not found", topicWordsAllocFile));
                return false;
            }

            int word = 0;
            this.topicWords = new float[this.NumTopics][];

            for (int i = 0; i < this.NumTopics; i++)
            {
                this.topicWords[i] = new float[this.NumWords];
            }

            try
            {
                using (var txtReader = new StreamReader(topicWordsAllocFile))
                {
                    // skip headers, 11 lines
                    for (int i = 0; i < TopicWordsAllocationHeaderLines; i++)
                    {
                        txtReader.ReadLine();
                    }

                    while (!txtReader.EndOfStream && word < this.NumWords)
                    {
                        var line = txtReader.ReadLine();

                        var tokens = line.Split((char[])null, StringSplitOptions.RemoveEmptyEntries);

                        for (int topic = 0; topic < this.NumTopics; topic++)
                        {
                            float x = 0f;
                            Single.TryParse(tokens[topic + 1], out x);
                            this.topicWords[topic][word] = x;                            
                        }

                        word++;
                    }
                }

                foreach (var topic in this.topicWords)
                {
                    var topicWordsSum = topic.Sum();                    
                    Enumerable.Range(0, topic.Length).Select(idx => topic[idx] = topic[idx] / topicWordsSum).Count();
                }

            using (var binWriter = new BinaryWriter(File.Create(topicWordsAllocBinFile)))
            {
                    // version
                    binWriter.Write(DataFileVersion);

                // number of topics
                binWriter.Write(this.NumTopics);

                // number of vocabularies
                binWriter.Write(this.NumWords);

                    for (int topic = 0; topic < this.NumTopics; topic++)
                    {
                        var bytes = new byte[this.NumWords * sizeof(float)];
                        Buffer.BlockCopy(topicWords[topic], 0, bytes, 0, bytes.Length);

                binWriter.Write(bytes);
                    }
                }
            }
            catch (Exception e)
            {
                StatusMessage.Write(string.Format("Error building topic words allocation: " + e.Message));
                return false;
            }

            return true;
        }

        /// <summary>
        /// Load topic words allocation from binary file
        /// </summary>
        /// <param name="topicWordsAllocBinFile">binary topic words allocation file</param>
        /// <returns>words allocation for each topics</returns>
        private bool LoadTopicWords(string topicWordsAllocBinFile)
        {
            if (!File.Exists(topicWordsAllocBinFile))
            {
                return false;
            }

            using (var binReader = new BinaryReader(File.OpenRead(topicWordsAllocBinFile)))
            {
                var version = binReader.ReadInt32();
                if (version != DataFileVersion)
                {
                    return false;
                }

                int topics = binReader.ReadInt32();
                int words = binReader.ReadInt32();

                this.topicWords = new float[topics][];

                for (int i = 0; i < topics; i++)
                {
                    topicWords[i] = new float[words];

                    var bytes = binReader.ReadBytes(words * sizeof(float));

                    Buffer.BlockCopy(bytes, 0, topicWords[i], 0, bytes.Length);
                }
            }

            return true;
        }

        public double Perplexity(string docVectorsFileName, string featurizedTestDocumentsFilename)
        {
            StatusMessage.Write(string.Format("Perpexity metric: Starting..."));
            var featurizedDocuments = LDAMetrics.LoadFeaturizedDocuments(featurizedTestDocumentsFilename);
            var documentVectors = this.LoadDocumentVectors(docVectorsFileName).ToArray();

            int countOfEffectiveWords = 0;
            double totalLogProbWordVector = 0.0D;
            int docId = 0;
            foreach (var featurizedDocument in featurizedDocuments)
            {
                // Each "featurizedDocument" is an object of Dictionary<wordId, wordFrequency>. It being empty means no features for this document.
                if (featurizedDocument.Count > 0)
                {
                    countOfEffectiveWords += this.CountEffectiveWords(featurizedDocument);
                    totalLogProbWordVector += this.SumLogProbWordVector(documentVectors[docId], featurizedDocument);
                }
                docId++;
            }

            if (countOfEffectiveWords == 0) return 1;
            return Math.Exp(-1.0D * (totalLogProbWordVector / countOfEffectiveWords));
        }

        public static IEnumerable<Dictionary<int, int>> LoadFeaturizedDocuments(string featurizedDocumentsFileName)
        {
            if (string.IsNullOrEmpty(featurizedDocumentsFileName) || !File.Exists(featurizedDocumentsFileName))
            {
                yield return null;
            }

            StatusMessage.Write(string.Format("Loading featurized documents - {0}", featurizedDocumentsFileName));

            foreach (var wordCounts in File.ReadLines(featurizedDocumentsFileName))
            {
                var documentWordCounts = new Dictionary<int, int>();

                var wordCountTupples = wordCounts.Split(' ');
                foreach (var wordIdCountPair in wordCountTupples)
                {
                    int separatorIndex = wordIdCountPair.IndexOf(':');
                    if (separatorIndex > 0)
                    {
                        int wordId = Convert.ToInt32(wordIdCountPair.Substring(0, separatorIndex));
                        int wordCount = Convert.ToInt32(wordIdCountPair.Substring(separatorIndex + 1));
                        documentWordCounts.Add(wordId, wordCount);
                    }
                }

                yield return documentWordCounts;
            }
        }

        IEnumerable<VectorBase> LoadDocumentVectors(string documentVectorsFileName)
        {
            StatusMessage.Write(string.Format("Loading document vectors - {0}", documentVectorsFileName));
            foreach (var line in File.ReadLines(documentVectorsFileName))
            {
                // Return an empty vector when line is empty, i.e. that test document does not contain any word in corpus vocabulary or is not of right language.
                yield return string.IsNullOrWhiteSpace(line) ? new DenseVector() : LoadDocumentVector(line);
            }
        }

        VectorBase LoadDocumentVector(string oneLineOfDocumentVectorsFile)
        {
            // Extract serialized doc vector from line: <docId> <locale> <corpus> <vector>.
            string vector = oneLineOfDocumentVectorsFile.Split(' ')[3];
            var denseVector = new DenseVector();
            denseVector.Deserialize(vector);
            return denseVector;
        }

        private int CountEffectiveWords(Dictionary<int, int> documentWordFrequencies)
        {
            if (documentWordFrequencies == null) return 0;
            var countOfEffectiveWords = 0;
            foreach (var wordFrequency in documentWordFrequencies.Values)
            {
                countOfEffectiveWords += wordFrequency;
            }

            return countOfEffectiveWords;
        }

        private double SumLogProbWordVector(VectorBase docVector, Dictionary<int, int> documentFeatures)
        {
            if (documentFeatures == null) return 0.0D;
            var totalLogProb = 0.0D;
            foreach (var wordId in documentFeatures.Keys)
            {
                totalLogProb += documentFeatures[wordId] * Math.Log(GetWordProbabilityGivenDocument(docVector, wordId));
            }

            return totalLogProb;
        }

        private double GetWordProbabilityGivenDocument(VectorBase docVector, int wordId)
        {
            double wordProbabilityGivenDocument = 0.0D;

            for (int topicK = 0; topicK < this.NumTopics; topicK++)
            {
                wordProbabilityGivenDocument += docVector[topicK] * this.topicWords[topicK][wordId]; 
            }

            return wordProbabilityGivenDocument;
        }

        /// <summary>
        /// Generate a brief summary of the model:  List each topic in the model, sorted (descending order) by the sum of its words' allocations.
        /// For each topic also list its most probable words (also in descending of probability)
        /// </summary>
        /// <param name="modelSynopsisFilename">The filepath of the output model synopsis</param>
        public int[] GenerateModelSynopsis(string modelSynopsisFilename)
        {
            if (string.IsNullOrEmpty(modelSynopsisFilename))
            {
                StatusMessage.Write("Generating Model Synopsis: Error - No Model Synopsis file name was specified");
                return null;
            }
    
            StatusMessage.Write("Generating Model Synopsis...");
            
            using (var file = new StreamWriter(modelSynopsisFilename, false, Encoding.UTF8))
            {
                file.WriteLine("Topics\t{0}", this.NumTopics);
                file.WriteLine("Good Topics\t{0}", this.topicsInfo.Count(x => !x.IsBadTopic));
                file.WriteLine("Vocabulary\t{0}", this.NumWords);
                file.WriteLine("Documents\t{0}", this.numDocs);
                file.WriteLine("Avg Topic Coherence\t{0}", this.avgTopicCoherence);
                file.WriteLine("Avg Good Topic Coherence\t{0}", this.avgGoodTopicCoherence);
                file.WriteLine("Avg Topic Specificity\t{0}", this.avgTopicSpecificity);
                file.WriteLine("Avg Good Topic Specificity\t{0}", this.avgGoodTopicSpecificity);
                file.WriteLine("Avg Topic Distinctiveness\t{0}", this.avgTopicDistinctiveness);
                file.WriteLine("Avg Good Topic Distinctiveness\t{0}", this.avgGoodTopicDistinctiveness);
                file.WriteLine("{0}\t{1}\t{2}\t{3}\t{4}\t{5}\t{6}\t{7}\t{8}", "Topic Id", "isBad", "Allocations", "Coherence", "Specificity", "Distinctiveness", "Prominent DF", "Top Docs", "High probability words");
                
                var topicIndexesAfterSorting =
                    Enumerable.Range(0, this.NumTopics)
                        .OrderByDescending(topic => this.topicsInfo[topic].AggregatedAllocation).ToArray();

                var badTopics = new int[this.NumTopics];
                var countBadTopics = 0;

                foreach (var topicId in topicIndexesAfterSorting)
                {
                    var words = this.GetTopWords(topicId, 50).Select(x => this.corpusVocabulary.Word(x));

                    if (this.topicsInfo[topicId].IsBadTopic)
                    {
                        badTopics[countBadTopics++] = topicId;
                    }

                    file.WriteLine(
                        "{0}\t{1}\t{2}\t{3}\t{4}\t{5}\t{6}\t{7}\t{8}",
                        topicId,
                        this.topicsInfo[topicId].IsBadTopic ? 1 : 0,
                        this.topicsInfo[topicId].AggregatedAllocation,
                        this.topicMetrics[topicId, (int)MetricType.Coherence],
                        this.topicMetrics[topicId, (int)MetricType.Specificity],
                        this.topicMetrics[topicId, (int)MetricType.Distinctiveness],
                        this.topicsInfo[topicId].ProminentFrequency,
                        string.Join(", ", this.topicsInfo[topicId].TopProminentDocuments.Select(x => string.Format("{0}|{1:0.000}", x.Item1, x.Item2))),
                        string.Join("\t", words));
                }

                return badTopics.Take(countBadTopics).ToArray();
            }
        }

        /// <summary>
        /// don't use
        /// </summary>
        /// <returns></returns>
        private double[,] LoadTopicWords()
        {
            var wordId = 0;
            StatusMessage.Write("Loading topic words...");
            var writer = StatusMessage.StatusWriter(() => string.Format("Processed {0:D0} words.", wordId));
            StatusMessage.Add(writer);

            var topicWords = new double[this.NumTopics, this.NumWords];

            var lines =
                File.ReadLines(this.wordTopicAllocationsFileName)
                    .Skip(TopicWordsAllocationHeaderLines)
                    .Take(this.NumWords);

            foreach (var line in lines)
            {
                var parts = line
                    .Split(' ')
                    .Skip(1)
                    .Take(this.NumTopics)
                    .Select(double.Parse)
                    .ToArray();

                for (var topic = 0; topic < this.NumTopics; topic++)
                {
                    topicWords[topic, wordId] = parts[topic];
                }

                wordId++;
            }

            StatusMessage.Remove(writer);
            return topicWords;
        }
    }
}
